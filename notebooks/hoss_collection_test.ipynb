{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmony HOSS Test Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "Before you beginning this tutorial, make sure you have an account in the Earthdata Login UAT or Production environment, which \n",
    "will be used for this notebook by visiting [https://uat.urs.earthdata.nasa.gov](https://uat.urs.earthdata.nasa.gov).\n",
    "These accounts, as all Earthdata Login accounts, are free to create and only take a moment to set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Authentication\n",
    "\n",
    "Below is a function to get a legacy echo token for cmr which is being phased out, best to input a token into notebook.\n",
    "Also set up netrc file locally to be able to download original granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import cmr\n",
    "import numpy as np\n",
    "from podaac.subsetter import subset\n",
    "from harmony import BBox, Client, Collection, Environment, Request\n",
    "import math\n",
    "import os \n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# GET TOKEN FROM CMR \n",
    "def get_token( url, client_id, user_ip, endpoint, username, password):\n",
    "    try:\n",
    "        token: str = ''\n",
    "        xml: str = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n",
    "        <token><username>{}</username><password>{}</password><client_id>{}</client_id>\n",
    "        <user_ip_address>{}</user_ip_address></token>\"\"\".format(username, password, client_id, user_ip)\n",
    "        headers: Dict = {'Content-Type': 'application/xml','Accept': 'application/json'}\n",
    "        resp = requests.post(url, headers=headers, data=xml)\n",
    "        \n",
    "        response_content: Dict = json.loads(resp.content)\n",
    "        token = response_content['token']['id']\n",
    "    except:\n",
    "        print(\"Error getting the token - check user name and password\", sys.exc_info()[0])\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a granule for subsetting\n",
    "\n",
    "Below we call out a specific granule (G1226018995-POCUMULUS) on which we will use the podaac L2 subsetter. Finding this information would complicate the tutorial- but po.daac has a tutorial available for using the CMR API to find collections and granules of interest. Please see the following tutorial for that information:\n",
    "\n",
    "PODAAC_CMR.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "collection = \"C2208422957-POCLOUD\"\n",
    "venue = 'OPS'\n",
    "input_token = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "cmr_root = 'cmr.earthdata.nasa.gov'\n",
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "edl_root = 'urs.earthdata.nasa.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = cmr.queries.CMR_OPS\n",
    "if venue == 'UAT':\n",
    "    cmr_root = 'cmr.uat.earthdata.nasa.gov'\n",
    "    harmony_root = 'https://harmony.uat.earthdata.nasa.gov'\n",
    "    edl_root = 'uat.urs.earthdata.nasa.gov'\n",
    "    mode = cmr.queries.CMR_UAT\n",
    "\n",
    "print (\"Environments: \")\n",
    "print (\"\\t\" + cmr_root)\n",
    "print (\"\\t\" + harmony_root)\n",
    "print (\"\\t\" + edl_root)\n",
    "\n",
    "if venue == \"UAT\":\n",
    "    username = os.environ.get(\"UAT_USERNAME\")\n",
    "    password = os.environ.get(\"UAT_PASSWORD\")\n",
    "elif venue == \"OPS\":\n",
    "    username = os.environ.get('OPS_USERNAME')\n",
    "    password = os.environ.get('OPS_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the above function to set up Earthdata Login for subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_url=\"https://\"+cmr_root+\"/legacy-services/rest/tokens\"\n",
    "if input_token is None:\n",
    "    token = get_token(token_url, 'jupyter', '127.0.0.1', edl_root, username, password)\n",
    "else:\n",
    "    token = input_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Subset of a PO.DAAC Granule\n",
    "\n",
    "We can now build onto the root URL in order to actually perform a transformation.  The first transformation is a subset of a selected granule.  _At this time, this requires discovering the granule id from CMR_.  That information can then be appended to the root URL and used to call Harmony with the help of the `request` library.\n",
    "\n",
    "Above we show how to find a granule id for processing.\n",
    "\n",
    "**Notes:**\n",
    "  The L2 subsetter current streams the data back to the user, and does not stage data in S3 for redirects. This is functionality we will be adding over time.\n",
    "  It doesn't work with URS backed files, which is coming in the next few weeks\n",
    "  it only works on the show dataset, but \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmr_url = \"https://\"+cmr_root+\"/search/granules.umm_json?collection_concept_id=\"+collection+\"&sort_key=-start_date\"\n",
    "headers = {'Authorization': token}\n",
    "response = requests.get(cmr_url, headers=headers)\n",
    "print(response)\n",
    "gid=response.json()['items'][0]['meta']['concept-id']\n",
    "print(response.json()['items'][0])\n",
    "print(gid)\n",
    "\n",
    "# Find Bounding box\n",
    "try:    \n",
    "    longitude_list = []\n",
    "    latitude_list = []\n",
    "    polygons = response.json()['items'][0]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry'].get('GPolygons')\n",
    "    lines = response.json()['items'][0]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry'].get('Lines')\n",
    "    if polygons:\n",
    "        for polygon in polygons:\n",
    "            points = polygon['Boundary']['Points']\n",
    "            for point in points:\n",
    "                longitude_list.append(point.get('Longitude'))\n",
    "                latitude_list.append(point.get('Latitude'))\n",
    "            break\n",
    "    elif lines:\n",
    "        points = lines[0].get('Points')\n",
    "        for point in points:\n",
    "            longitude_list.append(point.get('Longitude'))\n",
    "            latitude_list.append(point.get('Latitude'))\n",
    "            \n",
    "    if len(longitude_list) == 0 or len(latitude_list) == 0:\n",
    "        raise KeyError\n",
    "    north = max(latitude_list)\n",
    "    south = min(latitude_list)\n",
    "    west = min(longitude_list)\n",
    "    east = max(longitude_list)\n",
    "    \n",
    "except KeyError:\n",
    "    bounding_box = response.json()['items'][0]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['BoundingRectangles'][0]\n",
    "\n",
    "    north = bounding_box.get('NorthBoundingCoordinate')\n",
    "    south = bounding_box.get('SouthBoundingCoordinate')\n",
    "    west = bounding_box.get('WestBoundingCoordinate')\n",
    "    east = bounding_box.get('EastBoundingCoordinate')\n",
    "            \n",
    "#compute a smaller bounding box \n",
    "north = north - abs(.05 * (north - south))\n",
    "south = south + abs(.05 * (north - south))\n",
    "west = west + abs(.05 * (east - west))\n",
    "east = east - abs(.05 * (east - west))\n",
    "\n",
    "longitude = \"({}:{})\".format(west, east)\n",
    "latitude = \"({}:{})\".format(south, north)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download original granule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "granule_query = cmr.queries.GranuleQuery(mode=mode)\n",
    "granule_query.format('umm_json')\n",
    "granule_umm_json_url = granule_query.concept_id(gid).token(token)._build_url()\n",
    "response = requests.get(granule_umm_json_url)\n",
    "response_text = json.loads(response.content)\n",
    "urls = response_text.get('items')[0].get('umm').get('RelatedUrls')\n",
    "print(urls)\n",
    "\n",
    "def download_file(url):\n",
    "    local_filename = f\"{collection}_original_granule.nc\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f, length=int(4 * 1024 * 1024))\n",
    "    return local_filename\n",
    "\n",
    "for x in urls:\n",
    "    if x.get('Type') == \"GET DATA\" and x.get('Subtype') in [None, 'DIRECT DOWNLOAD'] and '.bin' not in x.get('URL'):\n",
    "        granule_url = x.get('URL')\n",
    "\n",
    "download_file(granule_url) \n",
    "print(granule_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lon, lat, time variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "group = None\n",
    "\n",
    "# Try to read group in file\n",
    "with Dataset(f'{collection}_original_granule.nc') as f:\n",
    "    for g in f.groups:\n",
    "        ds = xr.open_dataset(f'{collection}_original_granule.nc', group=g)\n",
    "        if len(ds.variables):\n",
    "            group = g\n",
    "        ds.close()\n",
    "        \n",
    "#ds = xr.open_dataset('original_granule.nc', engine=\"netcdf4\")\n",
    "if group:\n",
    "    ds = xr.open_dataset(f'{collection}_original_granule.nc', group=group)\n",
    "else:\n",
    "    ds = xr.open_dataset(f'{collection}_original_granule.nc')\n",
    "    \n",
    "lat_var = None\n",
    "lon_var = None\n",
    "\n",
    "# If the lat/lon coordinates could not be determined, use l2ss-py get_coord_variable_names\n",
    "if not lat_var or not lon_var:\n",
    "    try:\n",
    "        lat_var_names, lon_var_names = subset.compute_coordinate_variable_names(ds)\n",
    "        lat_var = lat_var_names[0]\n",
    "        lon_var = lon_var_names[0]\n",
    "    except ValueError:\n",
    "        for coord_name, coord in ds.coords.items():\n",
    "            if 'units' not in coord.attrs:\n",
    "                continue\n",
    "            if coord.attrs['units'] == 'degrees_north':\n",
    "                lat_var = coord_name\n",
    "            if coord.attrs['units'] == 'degrees_east':\n",
    "                lon_var = coord_name\n",
    "try:\n",
    "    time_var = subset.compute_time_variable_name(ds, ds[lat_var])\n",
    "except Exception as ex:\n",
    "    time_var = None\n",
    "    \n",
    "print(f'time_var={time_var}')\n",
    "print(f'lat_var={lat_var}')\n",
    "print(f'lon_var={lon_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Getting Variables for Collection\n",
    "\n",
    "We use the python-cmr library to query cmr for umm-v associated to the collection. The python-cmr library (https://github.com/nasa/python_cmr) provides us ways to query cmr for different umm data. Then we select a variable that is within the granule to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_query = cmr.queries.CollectionQuery(mode=mode)\n",
    "variable_query = cmr.queries.VariableQuery(mode=mode)\n",
    "\n",
    "collection_res = collection_query.concept_id(collection).token(token).get()[0]\n",
    "collection_associations = collection_res.get(\"associations\")\n",
    "variable_concept_ids = collection_associations.get(\"variables\")\n",
    "\n",
    "if variable_concept_ids is None and venue == 'UAT':\n",
    "    print('There are no umm-v associated with this collection and is uat env')\n",
    "    sys.exit(0)\n",
    "\n",
    "original_variable = None\n",
    "\n",
    "if lat_var is None or lon_var is None:\n",
    "    \n",
    "    variables_items = variable_query.concept_id(variable_concept_ids).format('umm_json').get_all()\n",
    "    variables_res = json.loads(variables_items[0]).get('items')\n",
    "    variables = [variable.get('umm').get('Name') for variable in variables_res]    \n",
    "    \n",
    "    for var in variables_res:\n",
    "        var_subtype = var.get('umm').get('VariableSubType')\n",
    "        if var_subtype == \"LONGITUDE\":\n",
    "            lon_var = var.get('umm').get('Name')\n",
    "        if var_subtype == \"LATITUDE\":\n",
    "            lat_var = var.get('umm').get('Name')\n",
    "        if var_subtype == \"TIME\":\n",
    "            time_var = var.get('umm').get('Name')\n",
    "else:\n",
    "    variables_res = variable_query.concept_id(variable_concept_ids[0:50]).get_all()\n",
    "    variables = [variable.get('name') for variable in variables_res]\n",
    "             \n",
    "for x in variables:\n",
    "    try:\n",
    "        if x != lat_var and x != lon_var and x != time_var:\n",
    "            new_path = None\n",
    "            if group:\n",
    "                if group in x:\n",
    "                    new_path = \"/\".join(x.strip(\"/\").split('/')[1:])\n",
    "            \n",
    "            if new_path:\n",
    "                original_variable = x\n",
    "                var_ds = ds[new_path]\n",
    "            else:\n",
    "                var_ds = ds[x]\n",
    "            print(np.isnan(var_ds).all())\n",
    "            msk = np.logical_not(np.isnan(var_ds.data.squeeze()))\n",
    "            true_exist = np.all((msk == False))\n",
    "            if true_exist == False:\n",
    "                if new_path:\n",
    "                    variable = new_path\n",
    "                else:\n",
    "                    variable = x\n",
    "                print(f'variable={variable}')\n",
    "                break\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "if ds[variable].size == 0:\n",
    "    print(\"No data in subsetted region. Exiting\")\n",
    "    sys.exit(0)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if venue == \"UAT\":\n",
    "    harmony_client = Client(env=Environment.UAT, auth=(username, password))\n",
    "elif venue == \"OPS\":\n",
    "    harmony_client = Client(env=Environment.PROD, auth=(username, password))\n",
    "\n",
    "subset_variable = variable\n",
    "if original_variable:\n",
    "    subset_variable = original_variable\n",
    "\n",
    "collection_obj = Collection(id=collection)\n",
    "\n",
    "bounding_box = BBox(w=west, s=south, e=east, n=north)\n",
    "variables = [subset_variable]\n",
    "\n",
    "combined_request = Request(collection=collection_obj, spatial=bounding_box, granule_id=[gid], variables=variables)\n",
    "combined_request.is_valid()\n",
    "combined_job_id = harmony_client.submit(combined_request)\n",
    "\n",
    "print(f'Processing job: {combined_job_id}')\n",
    "\n",
    "print(f'\\n{combined_job_id}')\n",
    "\n",
    "print(harmony_client.status(combined_job_id))\n",
    "\n",
    "print('\\nWaiting for the job to finish')\n",
    "results = harmony_client.result_json(combined_job_id, show_progress=True)\n",
    "\n",
    "new_filename = None\n",
    "for filename in [file_future.result()\n",
    "                 for file_future\n",
    "                 in harmony_client.download_all(combined_job_id, overwrite=True)]:\n",
    "    print(f'Downloaded: {filename}')\n",
    "    new_filename = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the subsetting worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if group:\n",
    "    ds = xr.open_dataset(new_filename, engine=\"netcdf4\", group=group)\n",
    "else:\n",
    "    ds = xr.open_dataset(new_filename, engine=\"netcdf4\")    \n",
    "\n",
    "print(ds)\n",
    "var_ds = ds[variable]\n",
    "msk = np.logical_not(np.isnan(var_ds.data.squeeze()))\n",
    "\n",
    "llat = ds[lat_var]\n",
    "llon = ds[lon_var]\n",
    "\n",
    "lat_max = llat.max()\n",
    "lat_min = llat.min()\n",
    "\n",
    "lon_min = llon.min()\n",
    "lon_max = llon.max()\n",
    "\n",
    "lon_min = (lon_min + 180) % 360 - 180\n",
    "lon_max = (lon_max + 180) % 360 - 180\n",
    "\n",
    "if (lat_max <= north or np.isclose(lat_max, north)) and (lat_min >= south or np.isclose(lat_min, south)):\n",
    "    print(\"Successful Latitude subsetting\")\n",
    "elif np.isnan(lat_max) and np.isnan(lat_min):\n",
    "    print(\"Partial Lat Success - no Data\")\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "if (lon_max <= east or np.isclose(lon_max,east)) and (lon_min >= west or np.isclose(lon_min, west)):\n",
    "    print(\"Successful Longitude subsetting\")\n",
    "elif np.isnan(lon_max) and np.isnan(lon_min):\n",
    "    print(\"Partial Lon Success - no Data\")\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare original Granule to Subsetted Granule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if group:\n",
    "    original_ds = xr.open_dataset(f\"{collection}_original_granule.nc\", decode_times=False, decode_coords=False, engine=\"netcdf4\", group=group)\n",
    "    subsetted_ds = xr.open_dataset(new_filename, decode_times=False, decode_coords=False, engine=\"netcdf4\", group=group)\n",
    "else:\n",
    "    original_ds = xr.open_dataset(f\"{collection}_original_granule.nc\", decode_times=False, decode_coords=False, engine=\"netcdf4\")\n",
    "    subsetted_ds = xr.open_dataset(new_filename, decode_times=False, decode_coords=False, engine=\"netcdf4\")\n",
    " \n",
    "for in_var in subsetted_ds.data_vars.items():\n",
    "    #compare attributes\n",
    "    np.testing.assert_equal(in_var[1].attrs, original_ds[in_var[0]].attrs)\n",
    "    \n",
    "    # compare type and dimension names\n",
    "    is_empty = np.isnan(in_var[1])\n",
    "    if not is_empty.all():\n",
    "        assert in_var[1].dtype == original_ds[in_var[0]].dtype\n",
    "        assert in_var[1].dims == original_ds[in_var[0]].dims\n",
    "    else:\n",
    "        print(f\"{in_var[1].name} is empty so skip checking for types\")\n",
    "\n",
    "print(\"Done\")\n",
    "original_ds.close()\n",
    "subsetted_ds.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
